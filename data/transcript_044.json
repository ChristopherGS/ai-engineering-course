" Hey everyone, welcome to the Late in Space podcast. This is Alessio, partner and CTO of Residence at Decibel Partners. I'm joined by Michael Hosewicz, founder of Small AI. Today we have Dylan Patel and welcome. You are the author of the extremely popular SemiAnalysis blog. We both had a little bit of claim to fame in breaking details of GPT-4. George Hotz came on our pod and talked about the make sure of experts thing and then you had a lot more detail. I talked about the mixture of experts in January, it's just people didn't really notice it. You went into a lot more detail and I'd love to dig into some of that. Yeah, thank you so much. I've been doing consulting in the industry, semiconductor industry since 17. 2021 got bored and in November I started writing a blog and then like 2022 was good and started hiring folks for my firm and then all of a sudden 2023 happens and it's like the perfect intersection. I used to do data science but not like AI, not really like multi-variable progression is not AI right but also I've been involved in the semiconductor industry for a long long time posting about it online since I was 12 right and all of a sudden this all kind of came to fruition so it's cool to have the blog sort of blow up and that way. I used to cover semis at Belly Esny as well and it was for a long time was just a mobile cycle and then a little bit of PCs but like not that much and then maybe some cloud stuff you know like public cloud you know semiconductor stuff but it really wasn't anything until this this wave and I was actually listening to you on one of the previous podcasts that you've done and it was surprising that high performance computing also kind of didn't really take off like AI is just the first form of high performance computing that worked. One of the theses I've had for a long time that I think people haven't really caught on but it's coming to fruition now is that the largest tech companies in the world their software is important but actually having and operating a very efficient infrastructure is incredibly important and so you know people talk about you know hey Amazon is great AWS is great because yes it is easy to use and they've built all these things but behind the scenes they've done a lot on the infrastructure that is super custom that Microsoft Azure and Google Cloud just don't even match in terms of efficiency. If you think about the cost to rent out SSD space so the cost to rent you know offer a database service on top of that obviously a cost to rent out a certain level of CPU performance Amazon has a massive advantage there and likewise like Google spent all this time doing that in AI right with their TPUs and infrastructure there and optical switches and all this sort of stuff and so in the past it wasn't immediately obvious I think with AI especially like with how scaling laws are going it's like incredibly important for infrastructure is like so much more important and then like when you just think about software cost right like the cost structure of it there was always a bigger component of R&D and like SaaS businesses you know all over SF all these SaaS businesses did crazy good because you know they just start as they grow and then all of a sudden they're so freaking profitable for each incremental new customer and AI software looks like it's going to be very different in my opinion right like the R&D cost is much lower in terms of people but the cost of goods sold in terms of actually operating the service I think will be much higher and so in that same sense infrastructure matters a ton. I think you wrote once that training costs effectively don't matter. Yeah in my opinion I think that's a little bit spicy but yeah it's like training costs are irrelevant right like GPT-4 right like 20,000 A100s that's that's like I know it sounds like a lot of money. 500 million all in is that a reasonable estimate? The supercomputer it's it's oh it's slightly more but yeah I think the 500 million is a fair enough number I mean if you think about just the pre-training right three months 20,000 A100s at you know a dollar an hour is like that is way less than 500 million right but of course there's data and all this sort of stuff. Yeah so people that are watching this on YouTube they can see a GPU poor and a GPU rich on the table which is inspired by your yeah yeah your Google Gemini it's the world block post. One did you know that this thing was going to blow up so much Sam Alman even tweeted about it he said incredible Google got the semi-analysis guy to publish their internal marketing recruiting chart and yeah tell people who are the GPU poor or the GPU rich like what's this framework that you think about? You know some of this work we've been doing for a while is just on infrastructure and like hey like when something happens I think it's like a sort of competitive advantage of our firm right me myself and my colleagues is we go from software all the way through to like low level manufacturing it's like who you know oh Google's actually ramping up TPU production massively right and like I think people in AI would be like well duh but like okay like who who has the capability of figuring out the number well one you could just get Google to tell you but that they don't they won't tell you right that's like a very closely guarded secret and most people that work at Google DeepMind don't even know that number right two you go through the supply chain and see what they've placed in orders three is sort of like well who's actually winning from them hey oh Celestica is building these boxes wow oh interesting okay um this company's involved in testing for them oh okay oh this company's providing design IP to them okay that's very valuable in a monetary sense but you know you have to understand the whole technology stack but on the flip side right is well why is Google building all these what could they do with it and what does that mean for the world especially in SF right like I'm sure you folks have been to parties we just brag about how many GPS they have like it's happened to me multiple times where someone's just like I'm just witnessing a conversation where somebody from Meta is bragging about how many GPS they have versus someone from another firm and then it's like or like a startup person's like dude can you believe we just acquired we have 512 H100s coming online in August it's like oh cool like you know going through the supply chain it's like dude you realize there's 400,000 manufactured last quarter and like 530,000 this quarter being sold of H100s and it's like oh crap that's that's a lot that's a lot of GPUs but then like oh how does that compare to Google and like there's one way to look at the world which is just like hey scale is all you need like obviously data matters obviously all this stuff matters but given any data set a larger model will just do better I think it's going to be more expensive but it's going to do better okay there's all these GPUs going to production Nvidia is going to sell well over three million you know total GPUs next year over a million H100s this year alone there's a lot of GPU capacity coming online it's it's an incredible amount and well what are people doing what are people working on I think it's very important to like just think about what are people working on right what actually are you building that's that's going to advance what is monetizable but what also makes sense and so like a lot of people were doing things that I thought felt counterproductive right you know in a world where in less than a year there's going to be more than four million high-end GPUs out there you know we can talk about the concentration of those GPUs but if you're doing really valuable work as a good person right like you're contributing in some way should you be focused on like well I don't have access to any of those four million GPUs right I actually only have access to gaming GPUs should I focus on like being able to fine tune a model on that right like no it's not really that important or like should I be focused on batch one inference on a cloud GPU like no that's like pointless like why would you do batch size one inference on an H100 that's just like ridiculously dumb there's a lot of counterproductive work and at the same time there's a lot of things that people should be doing I mean obviously most people don't have resources right and I love the open source and I want the open source to win and I hate the people who want to like no we're x-lab and we think this is the only way you should do it and if people don't do it this way they should be regulated against it and all this kind of stuff I hate that attitude so I want the open source to win right like companies like Mistral and like what Meta are doing you know Mosaic and and you know all these folks together all these people doing you know huge stuff for the open source you know want them to succeed but it's like there's certain things that are you know like hyper focusing on leaderboards at hugging phase no like truthful QA is a garbage benchmark some of the models that are very high on there if you use it for five seconds you're like this is garbage there was things I wanted to say also you know we're in a world where compute matters a lot Google is going to have more compute than any other company in the world period by by like a large large factor it's just like framing it into that like mindset of like hey like what are the counterproductive things what do I think personally or what have people told me that are involved in this should they focus on the pace of acceleration from 2020 to 2022 is less than 2022 to 2024 you know GP2 to 4 2 to 4 is like 2020 to 2022 right is is less than I think from GPT 4 in 2022 which is when it was trained right to what OpenAI and Google and and Anthropic could do in 2025 right like I think the pace of acceleration is is increasing and it's just good to like think about you know that sort of stuff that makes sense and the chart that Sam mentioned is about yeah Google TPU B5's completely overtaking OpenAI by orders of magnitude let's talk about the TPU a bit we had Chris Landner on the show which I know you know he used to work on TensorFlow and Google and he did mention that the goal of Google is like make TPUs go fast with TensorFlow but then he also had a post about PyTorch stealing the thunder how do you see that changing if like now that a lot of the compute will be TPU based and Google wants to offer some of that to the public too Google internally and I think you know is obviously on Jax and XLA and all that kind of stuff externally like they've done a really good job wouldn't say like TPUs through PyTorch XLA is amazing but it's it's not bad right like some of the numbers they've shown some of the you know code they've shown for TPUv5e which is not the TPUv5 that I was referring to which is in the sort of the post the gpu poor post is referring to but TPUv5e is like the new one but it's mostly it's mostly an inference chip it's a small chip it's about half the size of a TPUv5 that chip you know you can get very good performance on of llama 70b inference very very good performance when you're using PyTorch and XLA now of course you're going to get better if you go Jax XLA but I think Google is doing a really good job after the restructuring of focusing on external customers too probably won't focus too much on TPUv5 for everyone externally but V5e we're also building a million of those right hey a lot of companies are using them right or will be using them because it's going to be an incredibly cheap form of compute I think the world of frameworks and all that right like that's obviously something a researcher should talk about not myself but you know the stats are clear that PyTorch is way way dominating everything but Jax is like doing well like there's external users of Jax the forever shouldn't be that the person doing PyTorch level code right that high should also be writing custom CUDA kernels right there should be you know different layers of abstraction where people hyper optimize and make it much easier for everyone to innovate on separate stacks right and then every once in a while someone comes through and pierces through the layers of abstraction and innovates across multiple or a group of people but I think frameworks are important compilers are important right Chris Lattner's what he's doing is really cool I don't know how it'll work but it's it's super cool and it certainly works on CPUs we'll see about accelerators likewise there's opening as triton like what they're trying to do there and you know everyone's really coalescing around triton third-party hardware vendors there's palis I don't want to mischaracterize it but you can write in palis and it'll go through you can lower level code and it'll work to TPUs and GPUs kind of like triton but it's like there's a backend for triton I don't know exactly everything about it but I think there's a lot of innovation happening on make things go faster right how do you how do you go burr because every single person working in ML it would be a travesty if they had to write like custom CUDA kernels always right like that would just slow down productivity but at the same time you kind of have to by the way I like to quantify things when you say make things go is there a target range of mfu that you typically talk about yeah there's there's sort of two metrics that I like to think about a lot right so in training everyone just talks about mfu right but then on inference right which I think is one LLM inference will be bigger than training or multimodal whatever bubble line inference will be bigger than training probably next year in fact at least in terms of GPUs deployed and the other thing is like you know what's the bottleneck when you're running these models the simple stupid way to look at it is training is you know there's six flops floating point operations you have to do for every byte you read in right every every parameter you read in so if it's fp8 then it's a byte if it's fp16 it's two bytes whatever right that on training but on inference side the ratio is completely different it's two to one right there's two flops per parameter that you read in and parameters maybe one byte right because it's fp8 or intake but then when you look at the GPUs the GPUs are very very different ratio the h100 has 3.35 terabytes a second of memory bandwidth and it has a thousand teraflops of fp16 bfloat 16 right so that ratio is like I'm sorry I'm going to butcher the math here and people think I'm dumb but 256 to one right call it 256 to one if you're doing fp16 and same applies to fp8 right because uh anyways per parameter parameter read to a number of floating point operations right because if you if you if you quantize further then you but you also get double the performance on that lower quantization you know that that that does not fit the hardware at all so if you're just doing llm inference at batch one then you're always going to be under utilizing the flops you're only paying for memory bandwidth and the way hardware is developing that ratio is actually only going to get worse h200 will come out soon enough which will help the ratio a little bit you know improve memory bandwidth more than improves flops just like the a 180 gig did versus the a 140 gig but then when the b100 comes out the flops are going to increase more than memory bandwidth and when future generations come out and the same with amd side right mi 300 versus 400 as you move on generations just due to fundamental like semiconductor scaling d ram memory is not scaling as fast as logic has been and you can do a lot of interesting things on the architecture so so you're going to have this problem get worse and worse and worse and so on training it's very you know who cares right because my flops are still my bottleneck most of i mean memory bandwidth is obviously a bottleneck but like well you know batch sizes are freaking crazy right like people train like two million batch sizes trivial right like that's what lama i think did lama 70b was two million batch size unlike you talk to someone at one of the frontier labs and they're like yeah just two million two million token batch size right that's crazy or sequence sorry but when you go to inference side well it's impossible to do one to do two million batch size also your latency would be horrendous if you tried to do something that crazy so you kind of have this like different problem where on training everyone just kept talking mfu model flop utilization right how many flops six times the number of parameters basically more or less and then what's the quoted number right so if i have three three hundred and twelve teraflops out of my a100 and i was able to achieve 200 that's all that's really good right you know some people are achieving higher right some people are even lower that's a very important like metric to think about now you have like people thinking mfu is like a security risk but on inference mfu is not nearly as important right it's it's memory bandwidth utilization you know batch one is you know what memory bandwidth can i achieve right because if as i increase batch from batch size one to four to eight to you know even 256 right is sort of where the crossover happens inference wise where it's flops limiting you more and more but like you you should have very high memory bandwidth utilization so when people talk about a100s like 60 mfu is decent on h100s it's more like 40 45 percent because the flops increased more than the memory bandwidth but people over time will probably get above 50 on on h100 on mfu on training but on inference it's not being talked about much but mbu model bandwidth utilization is the important factor right so of my 3.35 terabytes a second memory bandwidth i have on my h100 can i get two can i get three right that's the important thing and and right now if you look at everyone's inference stuff i i dogged on this in the gpu poor thing right but it's like hugging faces libraries are actually very inefficient like incredibly inefficient for inference you get like 15 mbu on on on some configurations like eight a100s and llama 70b you get like 15 which is just like horrendous because at the end of the day your latency is derived from what memory bandwidth you can effectively get right you know so if if you're doing llama 70 billion 70 billion parameters if you're doing it intake okay that's 70 gigabytes a second gigabytes you need to read for every single inference every single forward pass plus you know the attention but you know again we're simplifying it 70 gigabytes you need to read for every forward pass what is an acceptable latency for our user to have i would argue 30 milliseconds per token some people would argue lower right but at the very least you need to achieve human reading level speeds and probably a little bit faster because we'd like their skin to have a usable model for chat bot style applications there's other applications of course but chat bot style applications you want it to be human reading speed so 30 tokens per second 30 tokens per second is 33 or 30 tokens milliseconds per token is 33 tokens per second times 70 is let's say three times seven is 21 and then add to zero so 2100 gigabytes a second right to achieve human reading speed on llama 70b right so one you can never achieve llama 70b human reading speed on even if you had enough memory capacity on a model on on an a100 right even an even an h100 to achieve human reading speed right of course you couldn't fit it because it's 80 gigabytes versus 70 billion parameters so you're you're you're kind of butting up against the limits already 70 billion parameters being 70 gigabytes at nt8 or fpa you end up with one how do i achieve human reading level speeds right so if i go with two h100s then now i have you know call it six terabytes a second of memory bandwidth if i achieve just 30 milliseconds per token then i'm you know which is uh 33 tokens per second which is 30 times you know is three terabytes a second was it three times a 21 2.1 terabytes a second of memory bandwidth then i'm only at like 30 percent bandwidth utilization so i'm not using all my flops on batch one anyways right because 70 you know that the the flops that you're using there's tremendously low relative to inference and i'm not actually using a ton of the tokens on inference so with two h100s i only get 30 milliseconds a token that's a really bad result you should be striving to get you know so upwards of 60 percent and that's like 60 is kind of low too right like i've heard people getting 70 80 model bandwidth utilization and then you know obviously you can increase your batch size from there and your model bandwidth utilization will start to fall as your flops utilization increases but you know there you have to pick the sweet spot for where you want to hit on the latency curve for your user obviously as you increase batch size you get more throughput per gpu so that's more cost effective there's a lot of like things to think about there but i think those are sort of the two main things that people want to think about and there's obviously a ton with regards to like networking and inner gpu connection because most of the useful models run on a single gpu they can't run on a single gpu is there is there tpu equivalent of melanox the google tpu is like super interesting because google has been working with broadcom who's the number one networking company in the world right so melanox was nowhere close to number one they had a niche that they were very good at which was the network card the the card that you actually put on the server but they didn't do much they didn't have they weren't doing successfully in the switches right which is you know you connect all the networks cards to switches and then the switches to all the servers so melanox was not that great i mean it was good they were doing good and nvidia bought them you know 19 i believe or 18 but broadcom has been number one in networking for decade plus and google partnered with them on making the tpu right so google does a lot of the design especially on the ml hardware side on how you pass stuff around internally on the chip but broadcom does a lot on the network side right they specifically how to get really high connection speed between two chips right they've done they've done a ton there obviously google works ton there too but this is sort of google's like less less discussed partnership that's truly critical for them and why google's tried to get away from them many times their latest target to get away from broadcom is 2027 right but like you know that's that's four years from now chip design cycles four years so they already tried to get away in 2025 and that failed they have this equivalent of very high speed networking it works very differently than the way gpu networking does and that's important for people who code on a lower level i've seen this described as the ultimate rate limit on how big models can go it's not flops it's not memory it's networking like it has the lowest scaling law is like the lowest morse laws and i don't know what to do about that because no one else has any solutions yeah yeah so i think i think what you're referring to is that like network speed has increased slower much slower than the other than than flops yeah and bandwidth yeah yeah and yeah that's a tremendous problem in the industry right that's why nvidia bought a networking company so our broadcom is is is working with on google's chip right now but of course on metas metas internal ai chip which they're on the second generation of working on that and what's the main thing that meta is doing interesting is networking stuff right multiplying tensors is kind of there's a lot of people have made good matrix multiply units right but it's about like getting good utilization out of those and interfacing with the memory and interfacing with other chips really efficiently makes designing these ships very hard most of the startups obviously have not done that really well i think the startup's point is the most interesting right you mentioned companies that are gpu poor they raise a lot of money and there's a lot of startups out there that are gpu poor and they're not raise a lot of money what should they do how do you see like the space dividing are we just supposed to wait for like the the big labs to do a lot of this work with a lot of the gpus what's like the gpu poor is beautiful version of the of the article opening eye who everyone would be like oh yeah they have more gps than anyone else right but they have a lot less flops than google right that was the point of the like thing it is but not just them it's like okay you you know it's like a relative totem pole right now of course google doesn't use gpus as much for training and inference they do use some but mostly tpus so kind of like the whole point is that everyone is gpu poor because we're going to continue to scale faster and faster and faster and faster and compute will always be a bottleneck just like data will always be a bottleneck you can have the best data set in the world and you can always have a better one and same with you have the biggest compute system in the world you can but you'll always want a better one and so it's like there's things that like mishra they trained a freaking awesome model on relatively fewer gpus right and and now they're scaling up higher and higher and higher right there's a lot that the gpu poor can do though right we all have phones we all have laptops right there is a world for running gpus or models on device the replet folks are you know trying to do stuff like that their models they can't follow scaling laws right why because there is a fundamental limit to how much memory bandwidth and capacity you can get on a laptop or a phone you know i mentioned the ratio of flops to bandwidth on a gpu is actually really good compared to like a macbook or like a phone to run llama 70 billion requires two terabytes a second memory bandwidth 2.1 human reading speed yeah but my phone has like 50 gigabytes a second your laptop even if you have an m1 ultra has what like i don't remember like a couple hundred gigabytes a second of memory bandwidth you can't run llama 70b just by doing the classical thing so there's like there's stuff like speculative decoding you know together did something really cool and they put it in the open source of course medusa right like things like that that are you know they work on batch size one they don't work on batch size you know high and then so there's like the world of like cloud inference and so in the cloud it's all about what memory bandwidth and mfu i can achieve whereas on the edge i don't think google is going to deploy a model that i can run on my laptop to help me with code or help me with you know xyz they're always going to want to run it on the cloud for control or maybe they let it run on the device but it's like only their pixel phone you know it's kind of like a walled garden thing there's obviously a lot of reasons to do other things for security for openness to not be at the whims of a trillion dollar plus company who wants my data right you know there's a lot of stuff to be done there and i think folks like replator they open source their model right things like what like together i just mentioned right that developing medusa that didn't take much gpu at all right that's they're very well they do have quite a few gpus they've had a big announcement about having 4 000 h100s that's still relatively poor right when we're talking about hundreds of thousands of like the big labs like open ai and so on and so forth or millions of tpus like google but still they were able to develop medusa with probably just one server one server with eight gpus in it and its usefulness of something like medusa something like speculative decoding is is on device right and that's what like a lot of people can focus on you know people can focus on all sorts of things like that i don't i don't know right like a new model architecture right like are we only going to use transformers i'm pretty tilled to think like transformers are it right my hardware brain can only know something that loves hardware right people should continue to try and innovate on that right like you know asynchronous training right like that kind of stuff is like super super interesting i think it's tim de matters he had like the same guys killa yes he had the swarm paper and petal that research is super cool the universities will never have much compute but like hey to prepare to do things to you know all these sorts of stuff like they should try to build you know super large models like you look at what singhua university is doing in china actually they open sourced their model too i think the largest like bipyramid account at least open source models i mean of course they didn't train it on much data but it's like you know it's like still like you can do some still cool stuff like that i don't know i think there's a lot that people can focus on one scaling out a service to many many users distribution is very important so figuring out distribution right like figuring out useful fine tunes right like doing lms that open i will never make sorry for the crassness a porn dolly three right open source is doing crazy stuff with stable diffusion right right like i don't i don't mean uh yeah but it's like it's like and there's a legitimate market i think there's a couple companies who make tens of millions of dollars of revenue from lms or diffusion models for porn right or or you know that kind of stuff like i mean there's a lot of stuff that people can work on that will be successful businesses or doesn't even have to be a business but can advance humanity tremendously that doesn't require crazy scale how do you think about the depreciation of like the hardware versus the models if you buy a h100 sure the next year is gonna be better but like at least the hardware is good if you're spending a lot of money on like training a smaller model it might be like super obsolete in like three months and you got now all this compute coming online i'm just curious if like companies should actually spend the time to like you know fine tune them and like work on them with like the next generation is going to be out of the box so much better unless you're fine tuning for on device use i think fine tuning current existing models especially the smaller ones is a useless waste of time because the cost of inference is actually much cheaper than you think once you achieve good mbu and you batch at a decent size which any successful business in the cloud is going to achieve you know and then and then two fine tuning like people like oh you know this seven billion parameter model if you find tuner on a data set is almost as good as 3.5 right why don't you fine tune 3.5 and look at your performance right and like there is nothing open source that is anywhere close to 3.5 yet there will be people also don't don't quite crash and was supposed to be falcon 140 b it's it's less parameters than 3.5 and also uh i don't know about the exact token count but i believe it do we know the parameters of 3.5 um it's not 175 billion people keep saying this you know because we know three but we don't know 3.5 3.5 it's definitely smaller no it's bigger than 175 i think it's sparse moe i'm pretty sure and uh yeah you can you can do some like gating around the size of it by looking at their inference latency well what's the theoretical bandwidth if they're running it on this hardware and doing tensor parallel in this way so they have this much memory bandwidth and maybe they get maybe they're awesome and they get 90 memory bandwidth utilization i don't know that's upper bound and you can see the latency that 3.5 gives you especially at like off peak hours or if you do fine tuning and you have your if you have a private enclave like my azure will quote you latency sinking you can figure out how many parameters per forward path which i think is somewhere in the like 50 to 40 billion range but i could be very wrong that's just like my guess based on that sort of stuff you know 50 ish actually i think open source will have models of that quality i mean i assume mosaic or like meta will open source and mistral will be able to open source models of that quality now furthermore right like if you just look at the amount of compute obviously data is very important and the ability all these tricks and dials that you turn to be able to get good mfu and good mbo right like depending on inference or training is there's a ton of tricks but at the end of the day there's like 10 companies that have enough compute in one single data center to be able to beat gpt4 right like straight up like if not today within the next six months right 4 000 h100s is i think you need about 7 000 maybe and with some algorithmic improvements that have happened since gpt4 and some of the some data quality improvements probably like you could probably get to even like you know less than 7 000 h100s running for three months to beat gpt4 of course that's going to take a really awesome team but there's quite a few companies that are going to have that many right open source will match gpt4 but then it's like what about gpt4 vision or what about you know five and six and you know all these kind of stuff and like interact tool use and dolly and like that's the other thing is like there's a lot of stuff on tool use that the open source could also do that the the gpu poor could do i think there are some folks that are doing that kind of stuff agents and all that kind of stuff i don't know that's way over my head the agent stuff yeah it's over everyone's head one more question on this sort of gemini gpu rich essay we've we've had a very wide range of conversation already so it's hard to categorize but i tried to look for the mina eats the world document oh it's not public no no no no no you've read it yeah i read it so so gnome chazir is like i i don't know i think he's like the goat the goat yeah i think he's the goat in one year he published like switch transformers like some some attention is all you need obviously but he also did the speculative decoding stuff yeah exactly it's like it's like all this stuff that we were talking about today was like you know and obviously there's other people that are awesome that were you know helping and all that sort of stuff mina eats the world was basically he wrote an internal document around the time where google had mina right but it was like he wrote it and he was like basically predicting everything that's happening now which is that like large language models are going to eat the world right in terms of you know compute and he's like the total amount of deployed flops within google data centers will be dominated by large language models back then a lot of people thought he was like silly for that right like internally at google but you know now if you look at it it's like oh wait millions of tpus you're right you're right you're right okay we're totally getting dominated by like both you know gemini training and inference right like you know total flops being dominated by llms was completely right so my question was he had a bunch of predictions in there do you think there were any like underrated predictions that may not have yet have come true was he wrong on anything mina sucked right if you'd look at the total flops right you know parameters times tokens times six it's like tiny tiny fraction of gpt2 which came out just a few months later which was like okay so he was right about everything but like maybe he knew about gpt i have no clue open ai clearly was like way ahead of google on llm scaling even then people didn't really recognize it back in gpt2 days maybe or the number of people that recognized it was maybe hundreds tens so we talked about transformer alternatives the other thing is gpu alternatives the tpu is obviously one but there's cerebras there's graphcora there's madax lemurian labs there's a lot of them thoughts on what's real who's alive who's kind of like a zombie company walking you know i mean i mentioned like transformers or the architecture that went out but i think you know the number of people who recognized that in 2020 was you know as you mentioned probably hundreds right you know for natural language processing maybe in 2019 at least right you think about a chip design cycle it's like years right you know so so it's kind of hard to bet your architecture on the type of model that develops but what's interesting about all the first wave ai hardware startups you know this ratio of of memory capacity compute and memory bandwidth right everyone kind of made the same bet which is i have a lot of memory on my chip which is a really dumb because the models have grew way past that right even cerebris right i mean you know like i'm talking about like graph core it's called s ram which is the memory on chip much lower density but much higher speeds versus you know d ram memory off chip and so everyone was betting on more memory on chip and less memory off chip and to be clear right for image networks and models that are small enough to just fit on your chip that works that is the superior architecture but scale right scale scale scale scale nvidia was the only company that bet on the other side of more memory bandwidth and more memory capacity external also the right ratio of memory bandwidth versus capacity a lot of people like graph core specifically right they had a ton of memory on chip and then they had a lot more memory off chip but that memory off chip was a much lower bandwidth same applies to samanova same applies to cerebris they had no memory off chip but they they thought hey i'm gonna make a chip the size of a wafer right like you know those guys they're they're they're silly right hundreds of megabytes we have 40 gigabytes there's no you know and then oh crap models are way bigger than 40 gigabytes right you're kind of as as the ones that people deploy everyone bet on sort of the left side of this curve right the interesting thing is that these there's new age startups like lumaria like madax i won't get into what they're doing but they're making much more rational bets i don't know you know it's it's hard to say with a startup like it's going to work out right obviously there's tons of risk embedded but those folks you know jay duani of lumarium and like mike and and and renear they understand models they understand how they work and if transformers continue to reign supreme whatever innovations those folks are doing on hardware are going to need to be fitted for that or you have to predict what the model architecture is going to look like in a few years right you know and and hit that spot correctly so it's kind of a background on those but like now you look today hey intel bought nirvana which was navine rouse mosaic mls he started mosaic ml and sold at the databricks and was recently obviously leading llms and stuff their ai there intel bought that company from him and then shut it down and bought this other ai company and and now that company is kind of a you know got new chips they're going to release a better chip than the h100 within the next quarter or so amd they have a gpu mi 300 that will be better than the h100 in a quarter or so now this has nothing about how hard it is to program it but at least hardware wise on paper it's better why because it's you know a year and a half later right than in the h100 or a year later than the h100 of course and you know a little bit more time and all that sort of stuff but they're at least making similar bets on memory bandwidth versus flops versus capacity kind of following nvidia's elite the questions are like what is the correct bet for three years from now how do you engineer that and will those alternatives make sense the other thing is if you look at total manufacturing capacity right for this sort of bet right you need high bandwidth memory you need hpm and you need large five nanometer dies you know soon three nanometer whatever right you need both of those components and you need the whole supply chain to go through that we've written a lot about it but you know to simplify it nvidia has a little bit more than half and google has like 30 right through broadcom so it's like the total capacity for everyone else much lower and they're all sharing it right amazon's training and in frenchia microsoft's in-house chip and you know you go down the list it's like metas in-house chip and also amd and also all these all of these companies are sharing like a much smaller slice their chips are not as good or if they are there even though they're you know i mentioned intel and amd's chips are better that's only because they're throwing more money at the problem kind of right you know nvidia charges crazy prices i think everyone knows that their gross margins are insane amd and intel and and others will charge more reasonable margins and so they're able to give you more hpm and etc for a similar price and so that ends up letting them beat nvidia if you will but their manufacturing costs are twice that in some cases right in the case of amd their manufacturing cost for mi 300 or more than twice that of h100 and it only beats h100 by a little bit from you know performance stuff i've seen so it's like you know it's it's tough for anyone to like bet the farm on a alternative hardware supplier right like in my opinion like you should either just like be like you know a lot of like ex google startups are just using tpus right and and hey that's google cloud you know after moving the tpu team into the cloud team infrastructure they're much more aggressive on external selling and so you companies like even see companies like apple using tpus for training llms as well as gpus but either bet heavily on tpus because that's where the capacity is bet heavily on gpus of course and stop worrying about it and and leverage all this amazing open source code that is optimized for nvidia if you do bet on amd or intel or or any of these startups then you better make damn sure you're really good at low level programming and damn sure you also have a compelling business case and that the hardware supplier is giving you such a good deal that it's worth it and also by the way nvidia is releasing a new chip in you know they're going to announce it in march and they're going to release it and ship it q2 q3 next year anyways right and that chip will probably be three or four times as good right and maybe it'll cost twice as much or 50 more i hear it's three extra performance on an llm and 50 more expensive is what i hear so it's like okay nothing nothing is going to compete with that even if it is 50 more expensive right and then you're like okay well that kicks the can down further and then nvidia moving to a yearly release cycle so it's like very hard for anyone to catch up to nvidia really right are you know investing all this in other hardware like if you're microsoft obviously who cares if i spend 500 million dollars a year on my internal chip who cares if i spend 500 million dollars a year on amd chips right like if it lets me knock the price of nvidia gpus down a little bit puts the fear of god within jensen huang right like you know then then it is what it is right and and likewise you know with amazon and so on and so forth you know of course they hope is that their chips succeed or that they can actually have an alternative that is much cheaper than nvidia but to throw a couple hundred million dollars at a company you know as product is is completely reasonable and in the case of amd i think it'll be more than a couple hundred million dollars right but yeah i think i think alternative hardware is like it really does hit like sort of a peak hype cycle kind of end of this year early next year because all nvidia has is h 100 and then h 200 which is just better more more memory bandwidth higher memory capacity h 100 but that doesn't beat what you know amd are doing doesn't beat what you know even intel's gaudi 3 does but then very quickly after nvidia will crush them and then those other companies are going to take two years to get to their next generation you know so it's good it's just a really tough place and and no one decides you know the the main thing about hardware is like hey that bet i talked about earlier is like you know that's very oversimplified right just memory bandwidth flops and and the memory capacity there's a whole lot more bets there's a hundred different bets that you have to make and guess correctly to get good hardware not even have better hardware than nvidia get close to them and that takes understanding models really really well that takes understanding so many different aspects whether it's power delivery or cooling or design layout all this sort of stuff and it's like how many companies can do everything here right it's like i'd argue google probably understands models better than nvidia i don't think people would disagree i'm an nvidia understands hardware better than than google and so you end up with like google's hardware is competitive but like does amazon understand models better than nvidia i don't think so and does amazon better hardware understand hardware better than nvidia no i'm also of the opinion that the labs are useful partners they're convenient partners they're not going to buddy up as close as people think right i don't i don't even think like you know i expect in the next few years that the open ai microsoft probably falls apart too i mean i mean they'll still continue to use gps and stuff there but like i think that the level of closeness you see today is probably the closest they get at some point they become competitive if openly i becomes its own cloud the level of value that they deliver to the world if you talk to anyone there they truly believe it'll be tens of trillions if not hundreds of trillions of dollars right in which case obviously you know i know i know weird corporate structure aside you know this is the same playing field as companies like microsoft and google google wants to also deliver hundreds of trillions of dollars of value and it's like obviously you're competing and microsoft wants to do the same and you're going to compete in general right like these these lab partnerships are going to be nice but they're probably incentivized to uh you know hey nvidia you should you know can you can you design the hardware in this way and videos like no it doesn't work like that it works like this and they're like oh so this is the best compromise right like i think opening i would be stupid not to do that with nvidia but also with amd and but also hey like how much time and and microsoft's internal silicon but it's like how much time do i actually have right like you know should i do that should i spend all my you know super super smart people's time and limited you know this caliber of person's time doing that or should they focus on like hey can we get like asynchronous training to work or like you know figure out this next multimodal thing or i don't know i don't know right like it's probably better you know hey can i eke out five percent more mfu and work on designing the next supercomputer right like these kind of things how much more valuable is that right so it's like you know it's it's tough to see you know even open ai helping microsoft enough to get their knowledge of models so so so good right like microsoft's gonna announce their chip soon it's worse performance than the h100 but the cost effectiveness of it is better for microsoft internally just because they don't have to pay the nvidia tax but again like by the time they ramp it and and all these sorts of things and oh hey that only works on a certain size of models once you exceed that then it's actually you know again better for nvidia so it's like it's really tough for open ai to be like yeah we want to bet on microsoft right like and hey we have you know i don't know what's their number of people they have now like 700 people you know of which how many do low level code do i want to have separate code bases for this and this and this and this and you know it's like it's just like a big headache to i don't know i think it'd be very difficult to see anyone truly pivoting to anything besides a gpu and a tpu especially if you have if you need that scale and and that scale that the lab at least the labs right require is absurd google says millions right of tps open ai will save millions of gps right like i truly do believe they think that that number of next generation gps right like the numbers that we're going to get to are like i bet you i mean i don't know but i bet sam alton would say yeah we're going to build a hundred billion dollar supercomputer in three years or two years right like and like after gpt5 releases if he goes to the market and says like hey i want to raise a hundred billion dollars at 500 billion dollar valuation i'm sure the market would give it to him right like and then they build that supercomputer right like i mean like i think that's like truly the path we're on and so it's hard to hard to imagine yeah i don't know one point that you didn't touch on and taiwan companies are famously very chatty about the fruit company should we take apple seriously at all in this game or they're just in a different world altogether i respect their products but like i don't think apple will ever release a model that you can get to say really bad things there's all these jail breaks but also like as soon as they happen like you know it gets fed back into opening eyes like platform and it gets them it's it's like being public and open is accelerating their like ability to make a better and better model right like the rlhf and all this kind of stuff i i don't see how apple can do that structurally like as a company like the fruit company ships perfect products or like or else right that's why everyone loves iphones right like and and all these like open source firms and like all these folks are doing exactly that right building a bigger and better model every you know every few months and i don't know how apple gets on that train but you know at the same time there's no company that has more powerful distribution right are people in taiwan concerned that it will come to a point where china will just claim taiwan i think i think that a lot of people they're not super concerned but there's some people that are super concerned i think i think especially after like you know instability across the world and in europe and in the middle east and even africa if you look at any of the stuff they're building up it seems very clear and if you talk to a lot of people they they think china will invade taiwan in 27 and or 26 in april or or in september that's sort of the best time frames right like a lot of people believe that's what will happen right maybe the semi-analysis analyst point of view is is it feasible to build this capacity up in the u.s. uh no no right people don't understand how fragmented the semiconductor supply chain really is and how many monopolies there are the u.s. could absolutely shut down the chinese semiconductor supply chain they won't but and china could absolutely shut down the u.s. one actually by the way but more more relevantly right is like you know austria has two companies like the country of austria in europe has two companies that have super high market share and very specific technologies that are required for every single like like chip period right there is no chip that is less than seven nanometer that doesn't get touched by this one austrian company's tool right and there is no alternative and there's another austria you know it's it's and there's another austrian company likewise everything two nanometer and beyond will be touched by their tool it's like both of these companies are like doing well less than a billion dollars in revenue right so it's like you think it's so inconsequential no there's actually like three or four japanese chemical companies same same idea right it's like the the supply chain is so fragmented right like people only ever talk about where the fabs were where they actually get produced but it's like i mean tsmc in arizona right tsmc is building a fab in arizona it's it's quite a bit smaller than the fabs in in taiwan but even ignoring that those fabs don't have to ship everything to taiwan back anyways and also they have to get what's called a mask from taiwan and get sent to get sent to arizona and by the way there's these japanese companies that make these chemicals that need to ship to you know like t.o.k and shinetsu and you know it's like and and hey it needs this tool from austria no matter what it's like oh wow wait actually like the entire supply chain is just way too fragmented you can't like re-engineer and rebuild it on a snap right it's just like that it's just complex to do that semiconductors are more complex than any other thing that humans do without a doubt there's more people working in that supply chain with xyz backgrounds and more money invested every year and r&d plus capex you know it's like it's just by far the most complex supply chain that humanity has and to think that we could rebuild it in a few years is absurd. In an alternative universe the u.s kept morris chang and people that year right like it was just one guy yeah in an alternative universe texas instruments communicated to morris chang that he would become ceo and so he never goes to taiwan and you know blah blah blah right yeah no but i you know that's just also i think i think the world would probably be further behind in terms of technology development if that didn't happen right like technology proliferation is how you accelerate the pace of innovation right so the you know the dissemination to oh wow hey it's not just a bunch of people in oregon at intel that are leading everything right or you know hey a bunch of people in samsung korea right or since you taiwan right it's actually all three of those plus all these tool companies across the country in the the netherlands and and in japan and the u.s and you know it's it's millions of people innovating on a disseminated technology that's led us to get here right i don't even think you know if morris chang didn't go to taiwan would we even be at five nanometer would be at seven nanometer probably not right like there's innovations that that you know happened because of that right let's get a quick lightning round done semi-analysis branded one so the first one is what are like foundational readings that people that are listening today should read to get up to speed on like semis i think the easiest one is is the pie torch 2.0 and triton one that i did you know there's the advanced packaging series there's the google infrastructure supremacy piece i think that one's really a critical because it explains google's infrastructure quite a bit from networking through all that sort of history of the tpu a little bit maybe like amd's mi 300 piece talks a lot about the one that we did on that are very good and then obviously like you know like i don't know probably like chip wars like chris miller who doesn't recommend that book right it's really a good book right i mean like i would say gordon moore's book is freaking awesome because you got to think about right like you know llm scaling laws are like moore's law on crack right kind of like you know in a different sense like you know if you think about all of human productivity gains since the 70s is probably just off of the base of semiconductors and technology right of course of course people across the world are getting you know access to oil and gas and all this sort of stuff but like at least in the western world since the 70s everything has just been mostly innovated because of technology right oh we're able to build better cars because semiconductors enable us to do that or be able to build better software because we're able to connect everyone because semiconductors enabled that right so it's like that is like i think that's why it's the most important industry in the world but like seeing the frame of mind of what gordon moore has written you know he's got a couple you know papers books but etc right only the paranoids are by right like i think i think like that philosophy and thought process really translates to the now modern times except maybe you know humanity has been an exponential s curve and this is like another exponential s curve on top of that so i think that's probably a good good readings to do has there been an equivalent pivot so gordon like that classic tale was more of like his the pivot to memory from memory to logic yeah yeah and then was there is there has there been an equivalent pivot in in sammy's history of that magnitude i mean i mean like you know some people would argue that like you know jensen you know he basically didn't care about he only cared about you know like gaming and 3d professional visualization and like rendering and things like that until like he started to learn about ai and then all of a sudden he's going to like universities like you want some gpus here you go right like i think there's even stories of like you know not so long ago nurips when it used to have the more unfortunate name he would go there and just give away gps to people right like there's like stuff like that like you know very grassroots like pivoting the company now like you you look on gaming forums and it's like everybody's like oh nvidia doesn't even care about us they only care about ai and it's like yes you're right they only care they they mostly only care about ai and and the gaming innovations are only because of like they're putting more ai into it right it's like but also like hey they're doing a lot of ship design stuff with ai and you know i think i think that's like not i don't know if it's equivalent pivot quite yet but you know because the digital you know logic is a pretty big innovation but i think that's a big one and you know likewise it's like you know what did what did open ai do right what did they pivot how do they pivot they left the like a lot of most people left the culture of like google brain and deep mind and decided to build this like company that's crazy cool right like and does things in a very different way and like is innovating in a very different way so you consider that a pivot even though it's not inside google they're on a very different path with like the dota games and all that before they eventually found like gpt's as the as the thing so it was a full like started in 2015 and then like really pivoted in 2019 to be like all right with the gpt company yeah yeah if i could classify them i don't i'm sure there's open air people who are yelling at me right now okay so just a general question about you know i'm a fellow writer on unsubstacked you are obviously managing your consulting business while you're also publishing these amazing posts how do you what's your writing process how do you source info like when do you sit down and go like here's the theme for the week do you have a pipeline going out just anything you describe i'm thankful for my you know my teammates because they are actually awesome like and they're much more um you know directed focused to working on one thing you know or not one thing but a number of things right like you know someone who's just expert on x and y and z in the semiconductor supply chain so that really helps with the that side of the business i most of the times only write when i'm very excited or you know it's like hey like we should work on this and we should write about this so like you know one of the most recent posts we did was we explained the manufacturing process for 3d nan flash storage gate all around transistors and 3d d ram and all this sort of stuff because there's a company in japan that's going public kukasi electric right it's like okay well we should do a post about this and we should explain this but like it's like okay we you know and so myron he did all that work myron shea and most of the work and awesome but like usually it's like there's a few like very long in-depth backburner type things right like that took a long time took you know over a month of research and myron knows this stuff already really well right like there's stuff like that that we do and and that like builds up a body of work for our consulting and some of the reports that we sell that aren't you know newsletter posts but a lot of times the process is also just like well like mean it's the world is the culmination of reading that having done a lot of work on the supply chain around the tpu ramp and co-hosts and hpm capacities and all this sort of stuff to be able to you know figure out how many units and that google's ordering all sort of stuff and then like also like looking at like open source is like all just that all that culminated in like i wrote that in four hours right i sent it to a couple people and they're like no change this this this oh you know add this because that's really going to piss off you know the open source community i'm like okay sure and then posted it right so it's like there's no like specific process unfortunately like the most viral posts especially in the ai community are just like those kind of pieces rather than like the really deep deep like you know obviously like what was in the gemini eats the world post you know obviously hey like we we do deep work there's a lot more like factual not leaks you know it's just factual research hey we across the team we go to 40 plus conferences a year right all the way from like a photoresist conference to photo mask conference to a lithography conference all the way up to like ai conferences and you know all everything in between networking conferences and piecing everything across the supply chain so it's like that's like the true like work and like yeah i don't know it is sometimes bad to like have the infamousness of you know only people caring about this or the gp4 leak or the google has no moat leak right it's like but like you know that's just like stuff that comes along right you know it's really focused on like understanding the supply chain and how it's pivoting and who's the winners who's the losers what technologies are inflecting things like that where is the best place to invest resources you know sort of like stuff like that and accelerating or capturing value etc awesome and to wrap if you had a magic genie that could answer any question that would change your world view what question would you ask that's a tough one um like you operate based on a set of facts about the world right now then there's maybe some unknowns where you're like man if i really knew the answer to this one i would do so many things differently or i would think about things very differently so i'm of the view at least everything that we've seen so far is that large-scale training has to happen in an individual data center with very high speed networking now everything doesn't need to be all to all connected but you need very high speed networking between all of your uh your chips right i would love to know you know hey magic genie how can we build artificial intelligence in a way that it can use multiple data centers of resources where there is a significantly lower bandwidth between pools of resources right because that would instantly like one of the big bottlenecks is how much power and how many chips you can get into a single data center so like a google and open ai and entropic are working on this and i don't know if they've solved it yet but if they haven't solved it yet then what is the solution because that will like accelerate the scaling that can be done by not just like a factor of 10 but like orders of magnitude because there's so many different data centers right like if you you know across the world and you know oh if i could pick up you know if i could effectively use 256 gps on this little data center here and then with this big cluster here you know how can you make an algorithm that can do that like i think that would be like the number one thing i'd be curious to know if how what because that changes the world significantly in terms of how can we continue to scale it's amazing technology that people have invented over the last you know five years awesome well thank you so much for coming on you"